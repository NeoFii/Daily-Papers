## Daily-Papers

#### Field
- NLP
- CV
- LLM
- DeepLearning
- MLLM

### Reading List
| Date | Paper | is_Finished |
|----|----|----|
|5/10/2025|[Attention is All You Need](https://arxiv.org/abs/1706.03762)| Yes |
|5/11/2025|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)| Yes |
|5/12/2025|[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)| Yes |
|5/13/2025|[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)| No |

> As a newcomer to this field, my approach to reviewing the literature may not be the most systematic. I’d greatly appreciate any suggestions—please don’t hesitate to open an issue or reach out to me at 732779870@qq.com.
> I'd like to share what I have read a paper everyday, let's grow and thrive together, hand in hand.
